# Engineering Portfolio

Mechanical Engineering student focused on aerospace systems, thermal modeling, and applied machine learning. Hands-on experience across CubeSat payload testing, aerodynamic analysis, and medical image modeling.

---

<h2 align="center">Validated Low-Speed CFD of a Blended-Wing-Body Aircraft </h2>

<p align="center"><em>Aerodynamics â€¢ CFD verification (mesh/domain) â€¢ Coefficient extraction + clean post-processing</em></p>

<p align="center">
  <a href="assets/BWB%20Executive%20Summary (2).pdf">
    <img src="assets/BWB_Plane_RelPresIso4deg.png" width="82%">
  </a>
</p>

<p align="center">
  <em>Relative static pressure on the BWB upper surface (Î± = 4Â°, Vâˆ â‰ˆ 40 mph).</em><br>
  ğŸ“„ <a href="assets/BWB%20Executive%20Summary (2).pdf"><strong>Open the executive summary (PDF)</strong></a>
</p>

A blended-wing-body aircraft can reduce drag by generating lift with more of the airframe, not just the wings, which is why itâ€™s a recurring concept in efficiency-focused aircraft design. In this study, I used steady-state CFD in SOLIDWORKS Flow Simulation to estimate lift/drag coefficients across angle-of-attack and to produce clean, comparable flow/pressure visualizations while backing the results with basic verification (convergence, domain, and mesh sensitivity checks).

**Highlights**
- Quantified performance: AoA sweep (âˆ’2Â° â†’ +8Â° at ~40 mph); peak efficiency L/D â‰ˆ 7.3 near ~6Â°
- Credibility checks: convergence stability window; domain <0.5% effect; mesh mediumâ†’fine Î”CL â‰ˆ 1.25%
- Communication: fixed views/scales for honest cross-case comparison (velocity cuts + pressure maps)


### Selected figures

<p align="center"><strong>Streamwise velocity montage (Vx)</strong></p>
<p align="center">
  <img src="assets/13CombinedVelX_STACKED_0_4_8_PORTFOLIO%20%281%29.png" width="50%">
</p>

<p align="center"><strong>Surface relative pressure montage (0Â°, 4Â°, 8Â°)</strong></p>
<p align="center">
  <img src="assets/CombinedRelPres_montage_0_4_8_PORTFOLIO.png" width="92%">
</p>

<p align="center"><strong>Mesh sensitivity (Î± = 4Â° baseline)</strong></p>
<p align="center">
  <img src="assets/ES3_mesh_sensitivity.png" width="92%">
</p>

---

<h2 align="center">HyCUBE: CubeSat Thermal &amp; Instrumentation Payload</h2>

<p align="center"><em>Aerospace Systems + Instrumentation â€¢ Sensor calibration + validation â€¢ Flight-readiness testing</em></p>

<p align="center">
  <strong>Program:</strong> NASA Minnesota Space Grant Consortium Ã— University of Minnesota SmallSat Program<br>
  <strong>Role:</strong> Aerospace Systems Research Intern
</p>

<p align="center">
  <img src="assets/HyCubeINAir.png" width="48%">
  <img src="assets/hycube_mission_graphic.webp" width="48%">
</p>

Reliable temperature sensing is critical for thermal testing and flight readiness. I built a thermocouple calibration + validation workflow for HyCUBE (Hypersonic Configurable Unit Ballistic Experiment) to support sensor selection and pre-flight verification. The pipeline converts raw voltage/temperature logs into regression-based calibration fits with confidence bounds and evaluates measurement agreement using parity and Blandâ€“Altman analysis.

**Results snapshot**
- Estimated sensitivity (slope): 19 in â‰ˆ 44.51 ÂµV/Â°C (RÂ² â‰ˆ 0.499), 25 in â‰ˆ 33.84 ÂµV/Â°C (RÂ² â‰ˆ 0.377), 30 in â‰ˆ 45.05 ÂµV/Â°C (RÂ² â‰ˆ 0.542)
- Agreement vs reference: overall bias â‰ˆ 0.00Â°C with limits of agreement â‰ˆ Â±3.60Â°C (Blandâ€“Altman)
- Deliverable: calibration summary + figures exported as PNGs suitable for reports and review decks

### Selected figures

<p align="center"><strong>Validation views (parity + error)</strong></p>
<p align="center">
  <img src="assets/hycube_validation_2up.png" width="92%">
</p>

<p align="center"><strong>Agreement + sensitivity</strong></p>
<p align="center">
  <img src="assets/hycube_agreement_2up.png" width="92%">
</p>

<p align="center"><strong>Centered calibration fits + 95% confidence bands (shared axes)</strong></p>
<p align="center">
  <img src="assets/hycube_centered_fit_ci_shared_axes(1).png" width="86%">
</p>




**Key contributions**
- Designed thermocouple calibration experiments and produced decision-ready plots for sensor selection
- Implemented cold-junction compensation and regression-based calibration with confidence bounds
- Validated measurement behavior with parity + Blandâ€“Altman agreement (bias and limits-of-agreement)
- Automated analysis outputs (tables + PNG exports) to keep results reproducible and reviewable
- Supported high-altitude balloon flight operations: payload integration, ground-station setup, flight monitoring, recovery, and post-flight data validation

---

<h2 align="center">StrokeNet: Hemorrhage Detection on Non-Contrast Head CT (Mofrad Lab-Mentored ML Project)</h2>

<p align="center"><em>Medical Imaging â€¢ Applied Deep Learning â€¢ Interpretability + validation focus</em></p>

<p align="center">
  <img src="assets/gradcam_grid_imagenet_maskedcrop (1).png" width="86%">
</p>

Rapid identification of intracranial hemorrhage on non-contrast head CT can accelerate triage because hemorrhagic findings drive time-critical decisions and differ from ischemic pathways. This project was motivated by Mofrad Lab discussions on stroke imaging and a concrete question that came up in mentorship feedback: whether CT-native self-supervised pretraining can provide better representations than standard ImageNet pretraining for CT. I built a reproducible slice-level hemorrhage detection baseline and compared three initialization strategies, random initialization (scratch), ImageNet pretraining, and CT-native student/teacher self-supervised pretraining (JEPA), using a fixed supervised training pipeline and interpretability checks with Grad-CAM.

### 1-page summary + validation results

<p align="center">
  ğŸ“„ <a href="assets/_CT%20(Non-Contrast)%20Hemorrhage%20Detection%20with%20CT-Native%20Self-Supervised%20Pretraining%20(JEPA)%20vs%20ImageNet%20(1).pdf"><strong>Open the 1-page project summary (PDF)</strong></a>
</p>

**Validation (held-out 20% split, n = 240 slices)**  
Same supervised training recipe across runs; only the initialization changed.

<table align="center">
  <thead>
    <tr>
      <th>Initialization</th>
      <th>ROC AUC</th>
      <th>Sensitivity</th>
      <th>Specificity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ImageNet-pretrained</td>
      <td>0.878</td>
      <td>0.70</td>
      <td>0.89</td>
    </tr>
    <tr>
      <td>Scratch (random init)</td>
      <td>0.829</td>
      <td>0.79</td>
      <td>0.74</td>
    </tr>
    <tr>
      <td>JEPA (CT self-supervised)</td>
      <td>0.752</td>
      <td>0.85</td>
      <td>0.53</td>
    </tr>
  </tbody>
</table>



**What this shows**
- On this small RSNA subset and a lightweight JEPA run, ImageNet pretraining preserved the best overall AUC and specificity.
- CT-native JEPA increased sensitivity (fewer missed hemorrhage slices) but traded off specificity, which is useful as a baseline and a pointer toward longer pretraining and tighter SSL tuning.

**Engineering contributions**
- Built an end-to-end CT preprocessing and training pipeline (HU conversion/windowing, normalization, augmentation, stratified splits)
- Implemented balanced training for class imbalance and stable fine-tuning
- Produced interpretability visuals (Grad-CAM grids with artifact-aware cropping) to sanity-check model behavior
- Implemented and tested student/teacher self-supervised pretraining and compared initialization strategies

<h2 align="center">Automated Market-Open Execution Bot (UPRO/SPXU) â€” Reliability-First Trading System</h2>

<p align="center"><em>Systems engineering â€¢ Deterministic state machine â€¢ Guardrails + audit logs â€¢ Cloud-ready automation</em></p>

<p align="center">
  <em>Paper-trading implementation focused on robust execution, constraints, and monitoringâ€”not performance marketing.</em>
</p>

I built a reliability-first automation system that runs at the U.S. market open, observes two leveraged S&P 500 ETFs (UPRO, SPXU) for a fixed time window, selects the leader using deterministic rules, and executes a single cash-only order with explicit guardrails. The emphasis is on <strong>safe automation</strong>: constraints (PDT awareness, whole-share sizing), failure handling (timeouts, cancel stale orders), and auditability (timestamped state-transition logs).

<p align="center">
  <strong>System diagram (end-to-end execution flow)</strong>
</p>

```mermaid
flowchart TD
  A[Scheduler / Entry Point<br/>Runs at 09:30 ET] --> B[Preflight Checks<br/>Clock open? API reachable?]
  B -->|Fail| X[Fail-closed: exit safely<br/>Log reason]
  B -->|Pass| C[Morning Cleanup<br/>Flatten leftover UPRO/SPXU<br/>Cancel open orders]
  C --> D[PDT / Constraints Gate<br/>Equity + daytrade_count check]
  D -->|Blocked| X
  D -->|Pass| E[Observation Window (fixed)<br/>Sample UPRO + SPXU prices]
  E --> F{Decision Logic<br/>Spread â‰¥ threshold?}
  F -->|Yes| G[Winner selected early]
  F -->|No| H[Pick leader at window end]
  G --> I[Position Sizing<br/>Cash-only allocation<br/>Whole-share floor]
  H --> I
  I --> J[Order Routing<br/>Market buy winner]
  J --> K[Fill Confirmation<br/>Timeout + retry logic]
  K -->|Unfilled| X
  K -->|Filled| L[Hold Overnight<br/>State recorded in logs]
  L --> M[Next Run: Cleanup Sell<br/>Compute realized P&L from fill]
  
  subgraph Auditability
    N[Timestamped Logs<br/>open â†’ observe â†’ decide â†’ order â†’ fill â†’ hold â†’ cleanup]
  end
  A --> N
  C --> N
  E --> N
  F --> N
  J --> N
  K --> N
  M --> N

**Highlights**
- Deterministic decision logic: fixed open-time observation window + threshold-based winner selection with a clear end-of-window tie-break
- Constraint-aware execution: whole-share sizing, cash-only allocation, and a dynamic PDT guard (fail-closed if checks cannot be verified)
- Reliability guardrails: cancel-stale-orders, fill confirmation with timeouts, and â€œmorning cleanupâ€ to flatten leftover positions safely
- Auditability: timestamped logs at each state transition (open â†’ observe â†’ decide â†’ route order â†’ confirm fill â†’ hold) for post-run review
